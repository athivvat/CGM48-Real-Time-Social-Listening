{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nจากบทความ \"ลองเล่น Sentiment Analysis\"\\nโดย Nuipin Decimo\\nhttps://bit.ly/3cMgR5a\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "จากบทความ \"ลองเล่น Sentiment Analysis\"\n",
    "โดย Nuipin Decimo\n",
    "https://bit.ly/3cMgR5a\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ใช้ตัดคำภาษาไทย\n",
    "import deepcut\n",
    "# ใช้งาน regex\n",
    "import re\n",
    "# จัดการเกี่ยวกับ array\n",
    "import numpy as np\n",
    "# สำหรับทำ classify และทดสอบโมเดล\n",
    "from nltk import FreqDist, precision, recall, f_measure, NaiveBayesClassifier\n",
    "from nltk.classify import apply_features\n",
    "from nltk.classify import util\n",
    "# สำหรับสร้างชุดข้อมูลสำหรับ train และ test เพื่อทดสอบประสิทธิภาพ\n",
    "from sklearn.model_selection import KFold\n",
    "import collections, itertools\n",
    "# for save model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thai Sentiment Text Analysis\n",
    "# คลังข้อมูลสำหรับ Sentiment ภาษาไทย โดย นาย วรรณพงษ์ ภัททิยไพบูลย์\n",
    "# https://github.com/PyThaiNLP/lexicon-thai\n",
    "\n",
    "data_pos = [(line.strip(), 'pos') for line in open(\"pos.txt\", 'r', encoding=\"utf8\")]\n",
    "data_neg = [(line.strip(), 'neg') for line in open(\"neg.txt\", 'r', encoding=\"utf8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words (sentence):\n",
    "    return deepcut.tokenize(''.join(sentence.lower().split()))\n",
    "    \n",
    "sentences = [(split_words(sentence), sentiment) for (sentence, sentiment) in data_pos + data_neg + data_neutral]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_sentences(sentences):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in sentences:\n",
    "        all_words.extend(words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = FreqDist(wordlist)\n",
    "    word_features = [word[0] for word in wordlist.most_common()]\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dq/m8glhnz94rlfmbkyprpy7j1xpyj1yf/T/ipykernel_13259/2208305365.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features_data = np.array(sentences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1071 test: 120\n",
      "=================== Results ===================\n",
      "Accuracy 0.733333\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.729412     0.794326]\n",
      "Precision  [0.885714     0.666667]\n",
      "Recall     [0.620000     0.982456]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.655462\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.640000     0.729730]\n",
      "Precision  [0.923077     0.580645]\n",
      "Recall     [0.489796     0.981818]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.638655\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.583333     0.723684]\n",
      "Precision  [0.875000     0.578947]\n",
      "Recall     [0.437500     0.964912]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.705882\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.674419     0.769231]\n",
      "Precision  [0.828571     0.662651]\n",
      "Recall     [0.568627     0.916667]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.630252\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.548387     0.725000]\n",
      "Precision  [0.850000     0.585859]\n",
      "Recall     [0.404762     0.950820]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.764706\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.800000     0.808219]\n",
      "Precision  [0.888889     0.710843]\n",
      "Recall     [0.727273     0.936508]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.655462\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.631579     0.744828]\n",
      "Precision  [0.827586     0.600000]\n",
      "Recall     [0.510638     0.981818]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.714286\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.685714     0.784314]\n",
      "Precision  [0.888889     0.659341]\n",
      "Recall     [0.558140     0.967742]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.731092\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.760870     0.770370]\n",
      "Precision  [0.833333     0.684211]\n",
      "Recall     [0.700000     0.881356]\n",
      "===============================================\n",
      "\n",
      "train: 1072 test: 119\n",
      "=================== Results ===================\n",
      "Accuracy 0.781513\n",
      "            Positive     Negative    Neutral\n",
      "F1         [0.765432     0.837838]\n",
      "Precision  [0.815789     0.765432]\n",
      "Recall     [0.720930     0.925373]\n",
      "===============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_data = np.array(sentences)\n",
    "# แบ่งข้อมูลเป็น 10 ชุด โดยไม่เรียง\n",
    "k_fold = KFold(n_splits=10, random_state=1992, shuffle=True)\n",
    "word_features = None\n",
    "accuracy_scores = []\n",
    "for train_set, test_set in k_fold.split(features_data):\n",
    "    word_features = get_word_features(get_words_in_sentences(features_data[train_set].tolist()))\n",
    "    train_features = apply_features(extract_features, features_data[train_set].tolist())\n",
    "    test_features = apply_features(extract_features, features_data[test_set].tolist())\n",
    "    classifier = NaiveBayesClassifier.train(train_features)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    for i, (feats, label) in enumerate(test_features):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    accuracy_score = util.accuracy(classifier, test_features)\n",
    "    print('train: {} test: {}'.format(len(train_set), len(test_set)))\n",
    "    print('=================== Results ===================')\n",
    "    print('Accuracy {:f}'.format(accuracy_score))\n",
    "    print('            Positive     Negative    Neutral')\n",
    "    print('F1         [{:f}     {:f}]'.format(\n",
    "        f_measure(refsets['pos'], testsets['pos']),\n",
    "        f_measure(refsets['neg'], testsets['neg'])\n",
    "    ))\n",
    "    print('Precision  [{:f}     {:f}]'.format(\n",
    "        precision(refsets['pos'], testsets['pos']),\n",
    "        precision(refsets['neg'], testsets['neg'])\n",
    "    ))\n",
    "    print('Recall     [{:f}     {:f}]'.format(\n",
    "        recall(refsets['pos'], testsets['pos']),\n",
    "        recall(refsets['neg'], testsets['neg'])\n",
    "    ))\n",
    "    print('===============================================\\n')\n",
    "    f = open('my_classifier.pickle', 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW IT WORK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets=[('I love this car','positive'), \n",
    "    ('This view is amazing','positive'),\n",
    "    ('I feel great this morning','positive'),\n",
    "    ('I am so excited about the concert','positive'),\n",
    "    ('He is my best friend','positive')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets=[('I do not like this car','negative'),\n",
    "    ('This view is horrible','negative'),\n",
    "    ('I feel tired this morning','negative'),\n",
    "    ('I am not looking forward to the concert','negative'),\n",
    "    ('He is my enemy','negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "for(words,sentiment)in pos_tweets+neg_tweets:\n",
    "    words_filtered=[e.lower() for e in words.split() if len(e)>=3]\n",
    "    tweets.append((words_filtered,sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['love', 'this', 'car'], 'positive'),\n",
       " (['this', 'view', 'amazing'], 'positive'),\n",
       " (['feel', 'great', 'this', 'morning'], 'positive'),\n",
       " (['excited', 'about', 'the', 'concert'], 'positive'),\n",
       " (['best', 'friend'], 'positive'),\n",
       " (['not', 'like', 'this', 'car'], 'negative'),\n",
       " (['this', 'view', 'horrible'], 'negative'),\n",
       " (['feel', 'tired', 'this', 'morning'], 'negative'),\n",
       " (['not', 'looking', 'forward', 'the', 'concert'], 'negative'),\n",
       " (['enemy'], 'negative')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_sentences(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'car',\n",
       " 'view',\n",
       " 'feel',\n",
       " 'morning',\n",
       " 'the',\n",
       " 'concert',\n",
       " 'not',\n",
       " 'love',\n",
       " 'amazing',\n",
       " 'great',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'like',\n",
       " 'horrible',\n",
       " 'tired',\n",
       " 'looking',\n",
       " 'forward',\n",
       " 'enemy']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'contains(this)': True, 'contains(car)': True, 'contains(view)': False, 'contains(feel)': False, 'contains(morning)': False, 'contains(the)': False, 'contains(concert)': False, 'contains(not)': False, 'contains(love)': True, 'contains(amazing)': False, 'contains(great)': False, 'contains(excited)': False, 'contains(about)': False, 'contains(best)': False, 'contains(friend)': False, 'contains(like)': False, 'contains(horrible)': False, 'contains(tired)': False, 'contains(looking)': False, 'contains(forward)': False, 'contains(enemy)': False}, 'positive'), ({'contains(this)': True, 'contains(car)': False, 'contains(view)': True, 'contains(feel)': False, 'contains(morning)': False, 'contains(the)': False, 'contains(concert)': False, 'contains(not)': False, 'contains(love)': False, 'contains(amazing)': True, 'contains(great)': False, 'contains(excited)': False, 'contains(about)': False, 'contains(best)': False, 'contains(friend)': False, 'contains(like)': False, 'contains(horrible)': False, 'contains(tired)': False, 'contains(looking)': False, 'contains(forward)': False, 'contains(enemy)': False}, 'positive'), ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa2b4eadf805d27120a790f1141282b1eddd490204b31e8ad8571ddd24a2a23d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('streaming': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
